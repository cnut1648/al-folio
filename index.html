<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jiashu Xu 徐家澍 </title> <meta name="author" content="Jiashu Xu 徐家澍"> <meta name="description" content="Personal website for Jiashu Xu "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, jiashu, jiashu-xu"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8C%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cnut1648.github.io//"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%78%75%31@%67.%68%61%72%76%61%72%64.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-4093-2315" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=0uYehJsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2110519123" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/cnut1648" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jiashu-xu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/JiashuXu2" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/267/0701.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/files/Jiashu_Xu_CV.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" i style="height: 3em"> <span class="font-weight-bold">Jiashu</span> Xu 徐家澍 <img src="/assets/img/icon.jpg" style="height: 3em; float: right"> </h1> <p class="desc" style="margin-top: -3.5em; margin-bottom: 2em; font-style: italic;">Master student @ Harvard</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile-480.webp 480w,/assets/img/profile-800.webp 800w,/assets/img/profile-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/profile.jpg?75befdfb900bbf836be1939d45b5101b" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <style type="text/css">.ccimg{max-width:350px;width:100%;display:block;margin-left:auto;margin-right:auto}</style> <p>Hi there 👋</p> <p>I am a master student at <a href="https://www.harvard.edu/" rel="external nofollow noopener" target="_blank">Harvard University</a> studying computer science. Previously I double majored in applied mathematics and computer science at <a href="https://www.usc.edu/" rel="external nofollow noopener" target="_blank">USC</a>. I had the honor to work with Prof. <a href="https://himalakkaraju.github.io/" rel="external nofollow noopener" target="_blank">Hima Lakkaraju</a> at Harvard SEAS, and Prof. <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> at USC. I have spent a bit of time as research intern at Amazon Science team working on LLM and as student collaborator at Microsoft Research working on synthetic data generation.</p> <p>My current research interests is in <code class="language-plaintext highlighter-rouge">Reliable AI</code>. Particularly,</p> <ul> <li>AI safety [<a href="https://arxiv.org/pdf/2305.14710.pdf" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://arxiv.org/pdf/2311.09763.pdf" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://arxiv.org/abs/2401.12255" rel="external nofollow noopener" target="_blank">3</a>]</li> <li>Training AI that excels in low-resource regime, through indirect supervision [<a href="https://aclanthology.org/2023.acl-long.138.pdf" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://aclanthology.org/2022.naacl-main.190.pdf" rel="external nofollow noopener" target="_blank">2</a>] or synthetic data [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830463.pdf" rel="external nofollow noopener" target="_blank">3</a>, <a href="https://arxiv.org/abs/2206.09592" rel="external nofollow noopener" target="_blank">4</a>, <a href="https://arxiv.org/abs/2309.05956" rel="external nofollow noopener" target="_blank">5</a>, <a href="https://arxiv.org/pdf/2312.14216.pdf" rel="external nofollow noopener" target="_blank">6</a>]</li> <li>Explanation and how we can learn from explanation [<a href="https://proceedings.neurips.cc/paper/2021/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://www.nature.com/articles/s41746-022-00738-y" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://www.nature.com/articles/s41746-022-00738-y" rel="external nofollow noopener" target="_blank">3</a>]</li> </ul> <p>This is my girlfriend 😍<a href="https://www.linkedin.com/in/carmen-liang/" rel="external nofollow noopener" target="_blank">Carmen</a> and me</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cc_and_me-480.webp 480w,/assets/img/cc_and_me-800.webp 800w,/assets/img/cc_and_me-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/cc_and_me.jpg" class="img-fluid rounded z-depth-1 ccimg" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fingerprintv2-480.webp 480w,/assets/img/publication_preview/fingerprintv2-800.webp 800w,/assets/img/publication_preview/fingerprintv2-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/fingerprintv2.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="fingerprintv2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu2024instructional" class="col-sm-8"> <div class="title">Instructional Fingerprinting of Large Language Models</div> <div class="author"> <em>Jiashu Xu</em>, <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang*</a> , <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma*</a>, <a href="https://koh.pw/" rel="external nofollow noopener" target="_blank">Pang Wei Koh</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Oral</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2401.12255" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cnut1648.github.io/Model-Fingerprint/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://aclanthology.org/2024.naacl-long.180/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/cnut1648/Model-Fingerprint" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>NAACL 2024 Oral</p> </div> <div class="abstract hidden"> <p>The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (\eg restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2024instructional</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Instructional Fingerprinting of Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Wang, Fei and Ma, Mingyu Derek and Koh, Pang Wei and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Oral}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2401.12255}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CR}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/dreamdistribution-480.webp 480w,/assets/img/publication_preview/dreamdistribution-800.webp 800w,/assets/img/publication_preview/dreamdistribution-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/dreamdistribution.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="dreamdistribution.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://eccv.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Under Review</abbr> </div> </div> <div id="zhao2023dream" class="col-sm-8"> <div class="title">DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models</div> <div class="author"> <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a> , <a href="https://www.linkedin.com/in/mydcxiao/?locale=en_US" rel="external nofollow noopener" target="_blank">Yuhang Xiao*</a>, <em>Jiashu Xu*</em>, <a href="https://www.microsoft.com/en-us/research/people/xinyangjiang/" rel="external nofollow noopener" target="_blank">Xinyang Jiang</a>, <a href="https://www.microsoft.com/en-us/research/people/yifanyang/" rel="external nofollow noopener" target="_blank">Yifan Yang</a> , <a href="http://recmind.cn/" rel="external nofollow noopener" target="_blank">Dongsheng Li</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>In review of The European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.14216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://briannlongzhao.github.io/DreamDistribution/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/briannlongzhao/DreamDistribution" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>The popularization of Text-to-Image (T2I) diffusion models enables the generation of high-quality images from text descriptions. However, generating diverse customized images with reference visual attributes remains challenging. This work focuses on personalizing T2I diffusion models at a more abstract concept or category level, adapting commonalities from a set of reference images while creating new instances with sufficient variations. We introduce a solution that allows a pretrained T2I diffusion model to learn a set of soft prompts, enabling the generation of novel images by sampling prompts from the learned distribution. These prompts offer text-guided editing capabilities and additional flexibility in controlling variation and mixing between multiple distributions. We also show the adaptability of the learned prompt distribution to other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our approach through quantitative analysis including automatic evaluation and human assessment. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">zhao2023dream</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Brian Nlong and Xiao, Yuhang and Xu, Jiashu and Jiang, Xinyang and Yang, Yifan and Li, Dongsheng and Itti, Laurent and Ge, Yunhao and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{In review of The European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/BVS-480.webp 480w,/assets/img/publication_preview/BVS-800.webp 800w,/assets/img/publication_preview/BVS-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/BVS.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="BVS.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Highlight</abbr> </div> </div> <div id="ge2024behavior" class="col-sm-8"> <div class="title">BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <a href="https://www.linkedin.com/in/yihetang/" rel="external nofollow noopener" target="_blank">Yihe Tang*</a>, <em>Jiashu Xu*</em>, <a href="https://www.cemgokmen.com/" rel="external nofollow noopener" target="_blank">Cem Gokmen*</a> , <a href="https://www.chengshuli.me/" rel="external nofollow noopener" target="_blank">Chengshu Li</a>, <a href="https://wensi-ai.github.io/" rel="external nofollow noopener" target="_blank">Wensi Ai</a>, <a href="https://web.stanford.edu/~benjm/" rel="external nofollow noopener" target="_blank">Benjamin Jose Martinez</a>, <a href="https://www.linkedin.com/in/arman-aydin-915035185/" rel="external nofollow noopener" target="_blank">Arman Aydin</a>, <a href="https://www.linkedin.com/in/mona-anvari/" rel="external nofollow noopener" target="_blank">Mona Anvari</a>, <a href="https://scholar.google.ca/citations?user=u4S8E4UAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ayush K Chakravarthy</a>, <a href="https://kovenyu.com/" rel="external nofollow noopener" target="_blank">Hong-Xing Yu</a>, <a href="https://jdw.ong/" rel="external nofollow noopener" target="_blank">Josiah Wong</a>, <a href="https://scholar.google.com/citations?user=sqTh_dwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Sanjana Srivastava</a>, <a href="https://scholar.google.com/citations?hl=en&amp;user=jGwt3mcAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Sharon Lee</a>, <a href="https://scholar.google.com/citations?user=QRvXHNsAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Shengxin Zha</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a> , <a href="https://yunzhuli.github.io/" rel="external nofollow noopener" target="_blank">Yunzhu Li</a>, <a href="https://robertomartinmartin.com/" rel="external nofollow noopener" target="_blank">Roberto Martin-Martin</a> , <a href="https://aptx4869lm.github.io/" rel="external nofollow noopener" target="_blank">Miao Liu</a>, <a href="https://pzzhang.github.io/pzzhang/" rel="external nofollow noopener" target="_blank">Pengchuan Zhang</a> , <a href="https://ai.stanford.edu/~zharu/" rel="external nofollow noopener" target="_blank">Ruohan Zhang</a>, <a href="https://profiles.stanford.edu/fei-fei-li" rel="external nofollow noopener" target="_blank">Li Fei-Fei</a>, and <a href="https://jiajunwu.com/" rel="external nofollow noopener" target="_blank">Jiajun Wu</a> </div> <div class="periodical"> <em>In Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Highlight</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.09546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://behavior-vision-suite.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>CVPR 2024 Highlight</p> </div> <div class="abstract hidden"> <p>The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as "filled" and "folded"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ge2024behavior</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Tang, Yihe and Xu, Jiashu and Gokmen, Cem and Li, Chengshu and Ai, Wensi and Martinez, Benjamin Jose and Aydin, Arman and Anvari, Mona and Chakravarthy, Ayush K and Yu, Hong-Xing and Wong, Josiah and Srivastava, Sanjana and Lee, Sharon and Zha, Shengxin and Itti, Laurent and Li, Yunzhu and Martin-Martin, Roberto and Liu, Miao and Zhang, Pengchuan and Zhang, Ruohan and Fei-Fei, Li and Wu, Jiajun}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Highlight}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/can-nli-480.webp 480w,/assets/img/publication_preview/can-nli-800.webp 800w,/assets/img/publication_preview/can-nli-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/can-nli.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="can-nli.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu-etal-2023-nli" class="col-sm-8"> <div class="title">Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?</div> <div class="author"> <em>Jiashu Xu</em> , <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Association for Computational Linguistics (ACL)</em> , Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Oral</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2212.10784" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2023.acl-long.138.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/luka-group/nli_as_indirect_supervision" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>ACL 2023 Oral</p> </div> <div class="abstract hidden"> <p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2023-nli</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can {NLI} Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Ma, Mingyu Derek and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Oral}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.138}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.138}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2450--2467}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instruction-attack-480.webp 480w,/assets/img/publication_preview/instruction-attack-800.webp 800w,/assets/img/publication_preview/instruction-attack-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/instruction-attack.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="instruction-attack.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr> </div> </div> <div id="xu2023instructions" class="col-sm-8"> <div class="title">Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</div> <div class="author"> <em>Jiashu Xu</em> , <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>, <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em> , Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14710" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://cnut1648.github.io/instruction-attack/" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://aclanthology.org/2024.naacl-long.171/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2023instructions</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jiashu and Ma, Mingyu Derek and Wang, Fei and Xiao, Chaowei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/beyond-generation-480.webp 480w,/assets/img/publication_preview/beyond-generation-800.webp 800w,/assets/img/publication_preview/beyond-generation-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/beyond-generation.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="beyond-generation.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr> <abbr class="badge flex-fill" style="color:rebeccapurple !important;">Extension</abbr> </div> </div> <div id="ge2023beyond" class="col-sm-8"> <div class="title">Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <em>Jiashu Xu*</em>, <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a>, <a href="https://neelj.com/" rel="external nofollow noopener" target="_blank">Neel Joshi</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2309.05956</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.05956" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/gyhandy/Text2Image-for-Detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data generation into foreground object generation, and contextually coherent background generation. To generate foreground objects, we employ a straightforward textual template, incorporating the object class name as input prompts. This is fed into a text-to-image synthesis framework, producing various foreground images set against isolated backgrounds. A foreground-background segmentation algorithm is then used to generate foreground object masks. To generate context images, we begin by creating language descriptions of the context. This is achieved by applying an image captioning method to a small set of images representing the desired context. These textual descriptions are then transformed into a diverse array of context images via a text-to-image synthesis framework. Subsequently, we composite these with the foreground object masks produced in the initial step, utilizing a cut-and-paste method, to formulate the training data. We demonstrate the advantages of our approach on five object detection and segmentation datasets, including Pascal VOC and COCO. We found that detectors trained solely on synthetic data produced by our method achieve performance comparable to those trained on real data (Fig. 1). Moreover, a combination of real and synthetic data yields even much better results. Further analysis indicates that the synthetic data distribution complements the real data distribution effectively. Additionally, we emphasize the compositional nature of our data generation approach in out-of-distribution and zero-shot data generation scenarios. We open-source our code at https://github.com/gyhandy/Text2Image-for-Detection</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ge2023beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Xu, Jiashu and Zhao, Brian Nlong and Joshi, Neel and Itti, Laurent and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2309.05956}</span><span class="p">,</span>
  <span class="na">abbr2</span> <span class="p">=</span> <span class="s">{Extension}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/X-Norm-480.webp 480w,/assets/img/publication_preview/X-Norm-800.webp 800w,/assets/img/publication_preview/X-Norm-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/X-Norm.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="X-Norm.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://icmi.acm.org/" rel="external nofollow noopener" target="_blank">ICMI</a></abbr> </div> </div> <div id="yin2022x" class="col-sm-8"> <div class="title">X-Norm: Exchanging Normalization Parameters for Bimodal Fusion</div> <div class="author"> <a href="https://yufengyin.github.io/" rel="external nofollow noopener" target="_blank">Yufeng Yin*</a>, <em>Jiashu Xu*</em>, <a href="https://www.linkedin.com/in/tianxin-zu-81b13a220" rel="external nofollow noopener" target="_blank">Tianxin Zu</a>, and <a href="https://people.ict.usc.edu/~soleymani/" rel="external nofollow noopener" target="_blank">Mohammad Soleymani</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 International Conference on Multimodal Interaction (ICMI)</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3536221.3556581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Multimodal learning aims to process and relate information from different modalities to enhance the model’s capacity for perception. Current multimodal fusion mechanisms either do not align the feature spaces closely or are expensive for training and inference. In this paper, we present X-Norm, a novel, simple and efficient method for bimodal fusion that generates and exchanges limited but meaningful normalization parameters between the modalities implicitly aligning the feature spaces. We conduct extensive experiments on two tasks of emotion and action recognition with different architectures including Transformer-based and CNN-based models using IEMOCAP and MSP-IMPROV for emotion recognition and EPIC-KITCHENS for action recognition. The experimental results show that X-Norm achieves comparable or superior performance compared to the existing methods including early and late fusion, Gradient-Blending (G-Blend), Tensor Fusion Network, and Multimodal Transformer, with a relatively low training cost.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yin2022x</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{X-Norm: Exchanging Normalization Parameters for Bimodal Fusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yin, Yufeng and Xu, Jiashu and Zu, Tianxin and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2022 International Conference on Multimodal Interaction (ICMI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{605--614}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> <li><div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/neural-sim-480.webp 480w,/assets/img/publication_preview/neural-sim-800.webp 800w,/assets/img/publication_preview/neural-sim-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/neural-sim.png" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="neural-sim.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge flex-fill"><a href="https://eccv.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a></abbr> </div> </div> <div id="ge2022neural" class="col-sm-8"> <div class="title">Neural-Sim: Learning to Generate Training Data with NeRF</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge</a>, <a href="https://harkiratbehl.github.io/" rel="external nofollow noopener" target="_blank">Harkirat Behl*</a>, <em>Jiashu Xu*</em>, <a href="https://sgunasekar.github.io/" rel="external nofollow noopener" target="_blank">Suriya Gunasekar</a>, <a href="https://neelj.com/" rel="external nofollow noopener" target="_blank">Neel Joshi</a>, <a href="https://people.csail.mit.edu/yalesong/home/" rel="external nofollow noopener" target="_blank">Yale Song</a> , <a href="https://xinw.ai/" rel="external nofollow noopener" target="_blank">Xin Wang</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em> , Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.11368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830463.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/gyhandy/Neural-Sim-NeRF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Training computer vision models usually requires collecting and labeling vast amounts of imagery under a diverse set of scene configurations and properties. This process is incredibly time-consuming, and it is challenging to ensure that the captured data distribution maps well to the target domain of an application scenario. Recently, synthetic data has emerged as a way to address both of these issues. However, existing approaches either require human experts to manually tune each scene property or use automatic methods that provide little to no control; this requires rendering large amounts of random data variations, which is slow and is often suboptimal for the target domain. We present the first fully differentiable synthetic data pipeline that uses Neural Radiance Fields (NeRFs) in a closed-loop with a target application’s loss function. Our approach generates data on-demand, with no human labor, to maximize accuracy for a target task. We illustrate the effectiveness of our method on synthetic and real-world object detection tasks. We also introduce a new "YCB-in-the-Wild" dataset and benchmark that provides a test scenario for object detection with varied poses in real-world environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ge2022neural</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural-Sim: Learning to Generate Training Data with NeRF}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ge, Yunhao and Behl, Harkirat and Xu, Jiashu and Gunasekar, Suriya and Joshi, Neel and Song, Yale and Wang, Xin and Itti, Laurent and Vineet, Vibhav}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div></li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%78%75%31@%67.%68%61%72%76%61%72%64.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0003-4093-2315" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=0uYehJsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2110519123" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/cnut1648" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jiashu-xu" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/JiashuXu2" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://dblp.org/pid/267/0701.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </div> <div class="contact-note">Feel free to contact me in email! </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jiashu Xu 徐家澍. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: July 20, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SM6XF0TG75"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SM6XF0TG75");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar. see latest work on google scholar page.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/files/Jiashu_Xu_CV.pdf"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-emojis-are-hard-to-interpret-and-judges-are-having-headaches",title:"Emojis are hard to interpret, and judges are having headaches \ud83e\udd15",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/Emoji/"}},{id:"post-test-for-randomness",title:"Test for randomness",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/Test_for_rand.pdf"}},{id:"post-bp-for-convolutional-layer",title:"BP for convolutional layer",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/BP_for_conv_layer.pdf"}},{id:"post-platonic-solids-and-graphs",title:"Platonic solids and graphs",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/Platonic_solids.pdf"}},{id:"post-one-probability-puzzle-of-pointer",title:"One probability puzzle of pointer",description:"",section:"Posts",handler:()=>{window.location.href="/files/posts/One_prob_puzzle_of_ptr.pdf"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%78%75%31@%67.%68%61%72%76%61%72%64.%65%64%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0003-4093-2315","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=0uYehJsAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2110519123","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/cnut1648","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/jiashu-xu","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/JiashuXu2","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/267/0701.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>