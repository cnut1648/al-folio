---
layout: distill
title: Machine learning has biases, and why you should be cautious about
categories:
  - ML
  - bias
nav: true
authors:
  - name: Jiashu Xu
    url: "https://cnut1648.github.io/"
    affiliations:
      name: USC
  - name: Emily Artiano
    url: "https://dornsife.usc.edu/cf/faculty-and-staff/faculty.cfm?pid=1067799"
    affiliations:
      name: USC
toc:
  - name: What is machine learning bias?
  - name: Machine learning biases are worse than you might think
  - name: Why does machine learning bias happen?
  - name: What can you do to mitigate machine learning bias?

_styles: >
  .caption {
    font-size: 13px;
    color: rgba(0, 0, 0, 0.6);
  }
  .footnote-href {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }
---

Machine learning sounds fancy, but the truth is: you already had a lot of experience with it! Don’t believe me? If you are like me who uses speech-to-text functionality a lot, well, this engine is powered by machine learning; when you attach your resume for job applications, this resume will be scanned and filtered automatically by the hiring companies, which also involve machine learning.
Don’t you think machine learning has made impressive progress in recent years?
It sounds less exciting when the speech-to-text system [
failed to understand African American Dialect](https://www.pnas.org/doi/10.1073/pnas.1915768117), and [the resume filtering system that ended up discriminating against women](https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against).

Today I am not writing about how marvelous machine learning is and its many benefits. Instead, I want to say quite the opposite.
Despite the impressive advancement of machine learning in recent years, the danger is lurking: **machine learning has biases, and those biases will affect you, the user!**

<figure>
  <picture>
    <img src="/assets/img/posts/ML_Bias/ML_monster.png" alt="ML can be a monster">
  </picture>
  <figcaption class="text-center">
    Machine learning can be more dangerous then you might think.
  </figcaption>
</figure>

## What are machine learning biases?

Machine learning biases are when your speech-to-text system can recognize White Americans’ accents without a sweat yet struggle to understand your African American accents; are when your resume looks equally good with male applicants yet is rejected because you happen to be a woman. Biases occur when there is a performance disparity, i.e., some users suffer below-average performance than others. In other words, certain users are systematically excluded from the technology simply because of their inherent social identities, including gender, ethnicity, or sexual orientation. I hope we all agree that this is nothing thrilling.

## Machine learning biases are worse than you might think

The concept of machine developing intelligence is nothing new in sci-fi fiction or movies, but the real-world Artificial Intelligence (AI; machine learning is the core of AI) applications didn’t blossom until 2016 when [AlphaGo beat Lee Sedol in a five-game Go match](https://www.wikiwand.com/en/AlphaGo_versus_Lee_Sedol)
. You might wonder: even if machine learning has biases, only six years have passed since the premiere, and we are certainly not living in [a sci-fi “utopia” where humans are governed by computer intelligence](https://www.wikiwand.com/en/The_Skin_I_Live_In); what harm can machine learning biases bring?

Biases are not merely minor, neglectable flaws. Despite its short history, machine learning has become ubiquitous at an unprecedented pace. The US AI market grew from
[43 Million in 2014](https://www.prnewswire.com/news-releases/artificial-intelligence-ai-market-2016-2020---creating-models-and-mechanism-of-artificial-intelligence-is-a-major-challenge-for-the-potential-5-billion-market---research-and-markets-300250018.html) to [119 Billion in 2022](https://www.globenewswire.com/news-release/2022/04/19/2424179/0/en/Artificial-Intelligence-Market-Size-to-Surpass-Around-US-1-597-1-Bn-By-2030.html#:~:text=According%20to%20Precedence%20Research%2C%20the,38.1%25%20from%202022%20to%202030).
Machine learning has profoundly shaped our life: every phone is now equipped with a machine learning-powered facial recognization engine, [machine learning is embedded in
Google’s search engine](https://searchengineland.com/how-google-uses-artificial-intelligence-in-google-search-379746#:~:text=Since%202015%2C%20when%20Google%20introduced,Google%20presents%20to%20its%20searches) and social media platforms use machine learning to learn your preferences based on browsing history and recommend articles of similar taste.

For reasons I will explain later, _all_ machine learning algorithms are susceptible to biases. Facebook’s facial recognization algorithm performs poorly for African Americans, which [are misclassified as ’primates’ by the algorithm](https://thegrio.com/2021/09/04/facebook-black-men-labeled-primates/). Google’s search engine also shows gender bias: there are 1,023,000 more results for “male surgeons” (1,290,000) compared to “female surgeons” (267,000) at the time of writing.

<figure>
  <picture>
    <img src="/assets/img/posts/ML_Bias/CEO.jpg" alt="CEO">
  </picture>
  <figcaption class="text-center">
    <a href="https://www.washington.edu/news/2015/04/09/whos-a-ceo-google-image-results-can-shift-gender-biases/">
University of Washington researchers also found</a> that only 11% of Google image search results for “CEO” is woman, while the percentage of female US CEOs are 27%.
  </figcaption>
</figure>
Moreover, the feedback loop between your browsed articles and recommended articles becomes an echo chamber and filter bubbles, as found by DeepMind researchers. You will soon find yourselves surrounded by viewpoints that are highly correlated with yours, with little chance of being exposed to different viewpoints.
{% twitter https://twitter.com/DeepMind/status/1101514121563041792 %}
Given these machine learning recommendation algorithms control a large portion of how we consume information today <d-footnote>
Youtube users spend 
<span>
  <a class="footnote-href" href="https://www.wbur.org/onpoint/2019/04/08/youtube-toxic-content-extremism-bloomberg-safety-internet">700,000,000 hours each day watching recommended videos global-wise
</a></span>, and Facebook recommends news feed that gets on average <span><a class="footnote-href" href="https://www.fool.com/investing/2018/02/06/people-still-spend-an-absurd-amount-of-time-on-fac.aspx">950,000,000 hours of watch day per day</a></span>.
</d-footnote>, divisive messages are spreading rapidly on digital platforms due to online polarization. 
Matt Watson, a youtube user, found out that [youtube recommendation system made it easier for pedophiles to share child porn in the comments sections of “recommended” videos](
https://www.youtube.com/watch?v=O13G5A5w5P0). This streaming platform also has a scandal history of promoting [inappropriate kid content](https://www.wired.com/story/youtube-kids-parental-settings-safer/), [conspiracy theories](
https://www.wired.com/story/wired-guide-to-conspiracy-theories/
), and
[terroist propaganda](https://www.wired.com/story/to-curb-terrorist-propaganda-online-look-to-youtube-no-really/).

What I have shown you are the implicit impact of biases, but machine learning biases can have a direct far-reaching impact on thousands of people.

Machine learning algorithms are employed in the Healthcare domain and guide and allocate care for patients in US hospitals.
Practitioners have long sought an impartial judge that is capable of making well-educated decisions without prejudice.
Machine learning algorithms were supposed to be this perfect decision maker because no apparent preassumptions were coded. However, many efforts in this direction appear to be futile.

A report in 2019 showed that [the algorithm have “systematically discriminated against black people”](https://www.nature.com/articles/d41586-019-03228-6).
The algorithm was found to prioritize White Americans over colored patients with chronic and complicated symptoms such as diabetes or kidney problems.
This technology is meant to facilitate the healthcare procedure and provide appropriate help for those who are needest, yet the sweeping report concludes that this algorithm in turn reduces the proportion of black patients receiving extra help by more than half, and those missing out “potentially faced a greater chance of emergency room visits and hospital stays.” Ironically, those who are in the deepest desire for such technology are marginalized, becoming the ones who are the most likely to be excluded from the technology.

Similarly, [COMPAS](<https://www.wikiwand.com/en/COMPAS_(software)>), an algorithm used by US courts to assess the likelihood of a defendant becoming a recidivist, [was found](https://massivesci.com/articles/machine-learning-compas-racism-policing-fairness/#:~:text=In%202016%2C%20ProPublica%20reported%20that,was%20biased%20against%20Black%20defendants.)
to be biased against Black defendants. COMPAS would mark innocent Black people as a recidivist twice more likely compared to White defendants, while 20% more likely to mark White defenders as lower risk of reoffending while they did continue committing crimes.

## Why does machine learning bias happen?

I have already said so much about the impacts machine learning biases can bring, and I should like to briefly explain why machine learning algorithms are evitably susceptible to biases.

Machine learning is _learning from data_.
Therefore, the quality of learning algorithms depend on **data** that humans feed. Unfortunately, we as humans have our own cognitive biases, such as confirmation bias (we tend to search for and interpret information in a way of confirming our preexisting beliefs). Such human biases inevitably reflect on the data we collect, and further propagate to the machine learning algorithm and in effect poison them to behavous in a biased way.

<figure>
  <picture>
    <img src="/assets/img/posts/ML_Bias/ML%20Bias%20Propagation.png" alt="poison is propagate">
  </picture>
  <figcaption class="text-center">
    You can think of the human biases as poison, which diffuse to the algorithm and thus the prediction of the algorithms.
  </figcaption>
</figure>

For example, google “cooking in the kitchen images” and there are nine women in top ten retrieved images. Given those images to machine will mislead it to conclude that all cooking agents are women, and the single man cooking image as outlier. As the result, algorithms have little chance realize that man can cook as well.

<figure>
  <picture>
    <img src="/assets/img/posts/ML_Bias/cooking.png" alt="cooking">
  </picture>
  <figcaption class="text-center">
    An illustrative example of how, after seeing three images of women cooking, 
  machine learning algorithms overlook the possibility of a cooking man. Image taken from <a href="https://arxiv.org/pdf/1707.09457.pdf">this awesome paper</a>.
  </figcaption>
</figure>

## What can you do to mitigate machine learning bias?

Companies, hospitals, new, and other institutions are assisted by automated decision-making systems which make decisions, implicitly or explicitly, about your life. You don’t want this decision to be unfair.

Luckily, hope is not all lost. Researchers have been aware of this serious issue and have proposed several ways to combat it! <d-footnote>
I have also written a brief report of methods I have been following <span><a href="https://cnut1648.github.io/files/posts/WRIT_340.pdf">here</a></span>, check out if you are interested!</d-footnote>
Meanwhile, as users, you can also contribute to creating a fair dataset! If you are willing to commit to data collection, big tech institutions like <a href="https://allenai.org/">Allen AI</a> and <a href="https://research.google/teams/brain/">Google Brain</a> will periodically recruit annotators to contribute to their open-source data such as <a href="https://allenai.org/data/drop">DROP</a> and <a href="https://github.com/google-research-datasets/paws">PAWS</a>. Or, a much simpler way is: most of the machine learning powered services have provided users the ability to override the prediction of the machine. For example, say you figure the speech-to-text engine creates garbage text for your African American accent, then just click the “report” button and provide a correct transcript. All the user-provided feedback, after being manually inspected, will be added to the data the machine learning algorithm is using. Simple as it is ✌️!
