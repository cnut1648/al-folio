<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Jiashu Xu 徐家澍</title> <meta name="author" content="Jiashu Xu 徐家澍"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8C%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cnut1648.github.io//publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jiashu </span>Xu 徐家澍</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/files/Jiashu_Xu_CV.pdf">CV</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/can-nli-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/can-nli-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/can-nli-1400.webp"></source> <img src="/assets/img/publication_preview/can-nli.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="can-nli.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge mr-2"><a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a></abbr><abbr class="badge" style="color:rebeccapurple !important;">Oral</abbr> </div> </div> <div id="xu-etal-2023-nli" class="col-sm-8"> <div class="title">Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?</div> <div class="author"> <em>Jiashu Xu</em>, <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Association for Computational Linguistics (ACL)</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2212.10784" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2023.acl-long.138.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/luka-group/nli_as_indirect_supervision" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains.</p> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/instruction-attack-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/instruction-attack-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/instruction-attack-1400.webp"></source> <img src="/assets/img/publication_preview/instruction-attack.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="instruction-attack.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div> </div> <div id="xu2023instructions" class="col-sm-8"> <div class="title">Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models</div> <div class="author"> <em>Jiashu Xu</em>, <a href="https://derek.ma/" rel="external nofollow noopener" target="_blank">Mingyu Derek Ma</a>, <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang</a>, <a href="https://xiaocw11.github.io/" rel="external nofollow noopener" target="_blank">Chaowei Xiao</a>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2305.14710</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14710" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="abstract hidden"> <p>Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.</p> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/beyond-generation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/beyond-generation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/beyond-generation-1400.webp"></source> <img src="/assets/img/publication_preview/beyond-generation.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="beyond-generation.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"> <abbr class="badge mr-2"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr><abbr class="badge" style="color:rebeccapurple !important;">Extension</abbr> </div> </div> <div id="ge2023beyond" class="col-sm-8"> <div class="title">Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <em>Jiashu Xu*</em>, <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a>, <a href="https://neelj.com/" rel="external nofollow noopener" target="_blank">Neel Joshi</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2309.05956</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.05956" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/gyhandy/Text2Image-for-Detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data generation into foreground object generation, and contextually coherent background generation. To generate foreground objects, we employ a straightforward textual template, incorporating the object class name as input prompts. This is fed into a text-to-image synthesis framework, producing various foreground images set against isolated backgrounds. A foreground-background segmentation algorithm is then used to generate foreground object masks. To generate context images, we begin by creating language descriptions of the context. This is achieved by applying an image captioning method to a small set of images representing the desired context. These textual descriptions are then transformed into a diverse array of context images via a text-to-image synthesis framework. Subsequently, we composite these with the foreground object masks produced in the initial step, utilizing a cut-and-paste method, to formulate the training data. We demonstrate the advantages of our approach on five object detection and segmentation datasets, including Pascal VOC and COCO. We found that detectors trained solely on synthetic data produced by our method achieve performance comparable to those trained on real data (Fig. 1). Moreover, a combination of real and synthetic data yields even much better results. Further analysis indicates that the synthetic data distribution complements the real data distribution effectively. Additionally, we emphasize the compositional nature of our data generation approach in out-of-distribution and zero-shot data generation scenarios. We open-source our code at https://github.com/gyhandy/Text2Image-for-Detection</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/beyond-generation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/beyond-generation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/beyond-generation-1400.webp"></source> <img src="/assets/img/publication_preview/beyond-generation.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="beyond-generation.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div> </div> <div id="ge2022dall" class="col-sm-8"> <div class="title">Dall-e for detection: Language-driven context image synthesis for object detection</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge*</a>, <em>Jiashu Xu*</em>, <a href="https://scholar.google.com/citations?user=IhqFMeUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Brian Nlong Zhao</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>arXiv preprint</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2206.09592" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/gyhandy/Text2Image-for-Detection" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-toimage synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach decouples training data generation into foreground object mask generation and background (context) image generation. For foreground object mask generation, we use a simple textual template with object class name as input to DALL-E to generate a diverse set of foreground images. A foreground-background segmentation algorithm is then used to generate foreground object masks. Next, in order to generate context images, first a language description of the context is generated by applying an image captioning method on a small set of images representing the context. These language descriptions are then used to generate diverse sets of context images using the DALL-E framework. These are then composited with object masks generated in the first step to provide an augmented training set for a classifier. We demonstrate the advantages of our approach on four object detection datasets including on Pascal VOC and COCO object detection tasks. Furthermore, we also highlight the compositional nature of our data generation approach on out-of-distribution and zero-shot data generation scenarios.</p> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/X-Norm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/X-Norm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/X-Norm-1400.webp"></source> <img src="/assets/img/publication_preview/X-Norm.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="X-Norm.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://icmi.acm.org/" rel="external nofollow noopener" target="_blank">ICMI</a></abbr></div> </div> <div id="yin2022x" class="col-sm-8"> <div class="title">X-Norm: Exchanging Normalization Parameters for Bimodal Fusion</div> <div class="author"> <a href="https://yufengyin.github.io/" rel="external nofollow noopener" target="_blank">Yufeng Yin*</a>, <em>Jiashu Xu*</em>, <a href="https://www.linkedin.com/in/tianxin-zu-81b13a220" rel="external nofollow noopener" target="_blank">Tianxin Zu</a>, and <a href="https://people.ict.usc.edu/~soleymani/" rel="external nofollow noopener" target="_blank">Mohammad Soleymani</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 International Conference on Multimodal Interaction (ICMI)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3536221.3556581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Multimodal learning aims to process and relate information from different modalities to enhance the model’s capacity for perception. Current multimodal fusion mechanisms either do not align the feature spaces closely or are expensive for training and inference. In this paper, we present X-Norm, a novel, simple and efficient method for bimodal fusion that generates and exchanges limited but meaningful normalization parameters between the modalities implicitly aligning the feature spaces. We conduct extensive experiments on two tasks of emotion and action recognition with different architectures including Transformer-based and CNN-based models using IEMOCAP and MSP-IMPROV for emotion recognition and EPIC-KITCHENS for action recognition. The experimental results show that X-Norm achieves comparable or superior performance compared to the existing methods including early and late fusion, Gradient-Blending (G-Blend), Tensor Fusion Network, and Multimodal Transformer, with a relatively low training cost.</p> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/neural-sim-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/neural-sim-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/neural-sim-1400.webp"></source> <img src="/assets/img/publication_preview/neural-sim.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="neural-sim.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://eccv2022.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div> </div> <div id="ge2022neural" class="col-sm-8"> <div class="title">Neural-Sim: Learning to Generate Training Data with NeRF</div> <div class="author"> <a href="https://gyhandy.github.io/" rel="external nofollow noopener" target="_blank">Yunhao Ge</a>, <a href="https://harkiratbehl.github.io/" rel="external nofollow noopener" target="_blank">Harkirat Behl*</a>, <em>Jiashu Xu*</em>, <a href="https://sgunasekar.github.io/" rel="external nofollow noopener" target="_blank">Suriya Gunasekar</a>, <a href="https://neelj.com/" rel="external nofollow noopener" target="_blank">Neel Joshi</a>, <a href="https://people.csail.mit.edu/yalesong/home/" rel="external nofollow noopener" target="_blank">Yale Song</a>, <a href="https://xinw.ai/" rel="external nofollow noopener" target="_blank">Xin Wang</a>, <a href="http://ilab.usc.edu/itti/" rel="external nofollow noopener" target="_blank">Laurent Itti</a>, and <a href="https://vibhav-vineet.github.io/" rel="external nofollow noopener" target="_blank">Vibhav Vineet</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision (ECCV)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2207.11368" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830463.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/unist-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/unist-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/unist-1400.webp"></source> <img src="/assets/img/publication_preview/unist.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="unist.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://naacl.org/" rel="external nofollow noopener" target="_blank">NAACL</a></abbr></div> </div> <div id="huang-etal-2022-unified" class="col-sm-8"> <div class="title">Unified Semantic Typing with Meaningful Label Inference</div> <div class="author"> <a href="https://jyhuang36.github.io/" rel="external nofollow noopener" target="_blank">James Y. Huang</a>, <a href="https://scholar.google.com/citations?user=UcegV-cAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Bangzheng Li*</a>, <em>Jiashu Xu*</em>, and <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.01826" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://aclanthology.org/2022.naacl-main.190.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/luka-group/unist" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Semantic typing aims at classifying tokens or spans of interest in a textual context into semantic categories such as relations, entity types, and event types. The inferred labels of semantic categories meaningfully interpret how machines understand components of text. In this paper, we present UniST, a unified framework for semantic typing that captures label semantics by projecting both inputs and labels into a joint semantic embedding space. To formulate different lexical and relational semantic typing tasks as a unified task, we incorporate task descriptions to be jointly encoded with the input, allowing UniST to be adapted to different tasks without introducing task-specific model components. UniST optimizes a margin ranking loss such that the semantic relatedness of the input and labels is reflected from their embedding similarity. Our experiments demonstrate that UniST achieves strong performance across three semantic typing tasks: entity typing, relation classification and event typing. Meanwhile, UniST effectively transfers semantic knowledge of labels and substantially improves generalizability on inferring rarely seen and unseen types. In addition, multiple semantic typing tasks can be jointly trained within the unified framework, leading to a single compact multi-tasking model that performs comparably to dedicated single-task models, while offering even better transferability.</p> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 abbr"> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://www.nature.com/npjdigitalmed/" rel="external nofollow noopener" target="_blank">NPJ Digit Med</a></abbr></div> </div> <div id="ma2021dss" class="col-sm-8"> <div class="title">Dissection Gesture Sequence during Nerve Sparing Predicts Erectile Function Recovery after Robot-Assisted Radical Prostatectomy</div> <div class="author"> <a href="https://scholar.google.com/citations?user=lrFwJtAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Runzhuo Ma</a>, <em>Jiashu Xu</em>, Ivan Rodriguez, Gina DeMeo, Aditya Desai, <a href="https://lqtrinh.com/" rel="external nofollow noopener" target="_blank">Loc Trinh</a>, Jessica Nguyen, <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="external nofollow noopener" target="_blank">Anima Anandkumar</a>, Jim Hu, and <a href="https://researchers.cedars-sinai.edu/Andrew.Hung" rel="external nofollow noopener" target="_blank">Andrew Hung</a> </div> <div class="periodical"> <em>NPJ Digit Medicine</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.nature.com/articles/s41746-022-00738-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf//files/Dissection_Gesture_Abstract_v6.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row d-flex align-items-center"> <div class="col-md-4 abbr"> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://www.auajournals.org/" rel="external nofollow noopener" target="_blank">AUA</a></abbr></div> </div> <div id="ma2021dart" class="col-sm-8"> <div class="title">Dissection Assessment for Robotic Technique (DART) to Evaluate Nerve-Spare of Robot-Assisted Radical Prostatectomy</div> <div class="author"> <a href="https://scholar.google.com/citations?user=lrFwJtAAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Runzhuo Ma</a>, Alvin Hui, <em>Jiashu Xu</em>, Aditya Desai, Michael Tzeng, Emily Cheng, <a href="https://lqtrinh.com/" rel="external nofollow noopener" target="_blank">Loc Trinh</a>, Jessica Nguyen, <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="external nofollow noopener" target="_blank">Anima Anandkumar</a>, Jim Hu, and <a href="https://researchers.cedars-sinai.edu/Andrew.Hung" rel="external nofollow noopener" target="_blank">Andrew Hung</a> </div> <div class="periodical"> <em>American Urological Association Annual Conference (AUA),</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/files/DART_v5.pdf" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="https://www.auajournals.org/doi/10.1097/JU.0000000000002607.01" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row d-flex align-items-center"> <div class="col-md-4 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/salkg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/salkg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/salkg-1400.webp"></source> <img src="/assets/img/publication_preview/salkg.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="salkg.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="d-flex justify-content-center"><abbr class="badge mr-2"><a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div> </div> <div id="chan2021salkg" class="col-sm-8"> <div class="title">SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning</div> <div class="author"> <a href="https://aarzchan.com/" rel="external nofollow noopener" target="_blank">Aaron Chan</a>, <em>Jiashu Xu</em>, <a href="https://www.linkedin.com/in/boyuan-long-48a5ab16b/" rel="external nofollow noopener" target="_blank">Boyuan Long</a>, <a href="https://soumyasanyal.github.io/" rel="external nofollow noopener" target="_blank">Soumya Sanyal</a>, <a href="https://scholar.google.com/citations?user=wPYXV7gAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Tanishq Gupta</a>, and <a href="https://shanzhenren.github.io/" rel="external nofollow noopener" target="_blank">Xiang Ren</a> </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS),</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2104.08793" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.neurips.cc/paper/2021/file/9752d873fa71c19dc602bf2a0696f9b5-Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/INK-USC/SalKG" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Augmenting pre-trained language models with knowledge graphs (KGs) has achieved success on various commonsense reasoning tasks. However, for a given task instance, the KG, or certain parts of the KG, may not be useful. Although KG-augmented models often use attention to focus on specific KG components, the KG is still always used, and the attention mechanism is never explicitly taught which KG components should be used. Meanwhile, saliency methods can measure how much a KG feature (e.g., graph, node, path) influences the model to make the correct prediction, thus explaining which KG features are useful. This paper explores how saliency explanations can be used to improve KG-augmented models’ performance. First, we propose to create coarse (Is the KG useful?) and fine (Which nodes/paths in the KG are useful?) saliency explanations. Second, to motivate saliency-based supervision, we analyze oracle KG-augmented models which directly use saliency explanations as extra inputs for guiding their attention. Third, we propose SalKG, a framework for KG-augmented models to learn from coarse and/or fine saliency explanations. Given saliency explanations created from a task’s training set, SalKG jointly trains the model to predict the explanations, then solve the task by attending to KG features highlighted by the predicted explanations. On three commonsense QA benchmarks (CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can yield considerable performance gains – up to 2.76% absolute improvement on CSQA.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Jiashu Xu 徐家澍. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 17, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </body> </html>